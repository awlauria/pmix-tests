# PRRTE Tool/Debug CI Tests
This directory contains the testcases and scripts for PRRTE tool and debugger continuous integration tests.

## Contents
+ build.sh: Script to build all test cases and support code for these tests
+ run.sh: Script to run tests, required by existing CI test framework, which just invokes run.py
+ cirun.py: Script to run groups of tool/debug CI tests with differing requirements for number of task slots per node.
+ run.py: Script to run and validate all test cases, requires Python 3 (currently Python 3.6)
+ test-utils.c: stdio function wrappers for stdio calls in test cases and supporting functions used by wrappers
+ tcfilter.c: Program to rewrite test case output so it can be compared to baseline files
+ Other .c source files: Source code for test cases
+ \*.baseline: Baseline files, compared with current test case output to validate test case execution

## CI Environment Variables

The following environment variable(s) are to be defined when running the `build.sh` phases.

 * `CI_PRRTE_SRC` : Absolute path to the PRRTE source code (e.g., `$CI_PRRTE_SRC/examples`).

The run.py script requires the following environment variables to be set for multi-node tests

* `CI_NUM_NODES` : Number of nodes to use for multi-node tests.
* `CI_HOSTFILE` : Hostfile specifying number of nodes to use, with slots=&LT;n&GT; parameter specifying necessary slots per node.

The cirun.py script requires the following environment variables to be set

* `CI_NUM_NODES` : Number of nodes to use for multi-node tests
* `CI_HOSTFILE` : Hostfile specifying number of nodes to use, without the slots= parameter.

Note that the CI multi-node test baselines are generated with CI_NUM_NODES=3, so CI_NUM_NODES=3 must be specified when running those tests.

## Usage
The test cases can be run manually by running the **run.py** script while positioned within a copy of this directory. 
If no parameters are specified, all test cases are run. If a test case name is specified on the command line,
then only the test case matching the name specified on the command line is run.

This set of test cases can be run by the CI test framework which finds the **run.sh** script in this directory and runs it.

```
shell$ ./run.py
```

```
shell$ ./run.py direct
```

Since CI tests have differing requirements for slots per node, the cirun.py script can be run when running all CI testcases. The cirun.py script sets up a correct hostfile then runs the run.py script to run the subset of tests that require that hostfile.

## Background Info
The intent for these test cases is to implement a set of small tests that can verify the basic correctness of a specific part 
of PMIx or PRRTE functionality. The test cases should be small, run quickly, and generate consistent output that can
be compared to a baseline file. Tests which rely on the specific ordering of output, such as verifying that a request from
a tool is immediately followed by a response from a server process or daemon are outside the scope of this framework.

A baseline file is a file containing stdout or stderr output from a previous run of the
test case which has been determined to be correct.

The run.py script runs each test case that is specified in the **tests** array in that script. An entry in the tests array
names a testcase, specifies flags as needed to control execution of supporting processes, and contains the command line to
run the test. The run.py script supports limited symbolic substitution, where the script looks for and handles specific
substitutions as needed.

The run.py script can launch supporting processes needed by a test, such as persistent daemons and applications
to attach to. The run.py script also attempts to enforce timeouts to avoid the script hanging due to a test case which
does not terminate in a reasonable time, and attempts to clean up all related processes at the end of each test case
execution attempt.

Timeouts in this script can be controlled by the following environment variables:
+ **TC_TIMEOUT**: Specifies the maximum number of seconds the test case is allowed to run before timeout. Default 60 seconds.
+ **TC_WAIT_TIMEOUT**: Maximum number of seconds allowed for a wait() system call to wait when attempting cleanup of a process.
Default 5 seconds.
+ **TC_DAEMON_DELAY**: Number of seconds to allow for daemon initialization after it is spawned. Default 5 seconds.


In order for the baseline comparison approach to work, application output must be identical from run to the next. However,
application output is not always identical from one run to the next. Output may contain variable information such as
PMIx namespaces, process ids and hostnames. Also, output may not be identical due to differences in ordering for output 
generated by the multiple processes involved in the test.

The output ordering problem is handled by prefixing each line of test case output with a label which consists of a tag to 
identify the generating process and a sequential output line number. This prefixing is handles in a modile, test-utils.c 
that implements wrappers for each of the stdio output functions. These wrappers prefix the application output text with
the tag then call the underlying libc function to write the output. 

The wrappers use a lock file to try to prevent output lines from multiple processes being fragmented in the output file.
The locking may affect interprocess timings. There's probably also work to be done to make this work with multiple nodes.

When run.py runs the test, it writes application stdout and stderr to separate files. After the test case completes, run.py 
sorts each of the output files, which groups output by process tag and sequence number. This ensures that the validation
logic does not need to deal with the randomness of execution of multiple processes in the test, but also eliminates
the possility of validating that output from multiple processes occurs in a specific order.

The run.py script invokes the tcfilter utility to deal with variable information like namespaces in test case output.
The tcfilter program uses regular expressions to match the variable data, extract it from the output text and rewrite
it as a consistent string.

The tcfilter utility maintains a simple sequential array of strings to match for namespaces. As new unique strings of each class are found, they are added to the array.  The re-written text is a tag, such as **@NS&lt;n&gt;** where n is the array index of the original string.  Pids and hostnames, which also vary with each run are found and converted to @PID&LT;?&GT; and @HOST&LT;?&GT;
  
Test case output may not be structured to be easily machine parsable. Therefore, consideration should be given to
structuring that output so it can be more reliably parsed. Also, namespaces, hostnmes and pids should be formatted to match existing patterns whenever possible.

## Generating New Baselines
New baselines may be required if the CI test output changes as a result of bug fixes or functional changes.

There is a baseline file for stdout and stderr for each testcase. For instance the attach testcase has files named **attach.stdout.baseline** and **attach.stderr.baseline**

When a testcase is run, it generates output files for stdout and stderr, for instance **attach.stdout** and **attach.stderr**.

To update a bseline file, examine the corresponding new testcase output file to verify results are expected, and if the results are correct, then rename the testcase output file to the name of the corresponsing baseline file and commit the new baseline files to the git repository.

Note that if a tool/debugger testcase has its output changed, then the baseline files must be committed and merged to the test case repository master branch before CI test execution will be successful.
