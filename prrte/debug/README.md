# PRRTE Tool/Debug CI Tests
This directory contains the testcases and scripts for PRRTE tool and debugger continuous integration tests.

## Contents
+ build.sh: Script to build all test cases and support code for these tests
+ run.sh: Script to run tests, required by existing CI test framework, which just invokes run.py
+ run.py: Script to run and validate all test cases, requires Python 3 (currently Python 3.6)
+ test-utils.c: stdio function wrappers for stdio calls in test cases and supporting functions used by wrappers
+ tcfilter.c: Program to rewrite test case output so it can be compared to baseline files
+ Other .c source files: Source code for test cases
+ *.baseline: Baseline files, compared with current test case output to validate test case execution

## Usage
The test cases can be run manually by running the **run.py** script while positioned within a copy of this directory. 
If no parameters are specified, all test cases are run. If a test case name is specified on the command line,
then only the test case matching the name specified on the command line is run.

This set of test cases can be run by the CI test framework which finds the **run.sh** script in this directory and runs it.

```
shell$ ./run.py
```

```
shell$ ./run.py direct
```

## Background Info
The intent for these test cases is to implement a set of small tests that can verify the basic correctness of a specific part 
of PMIx or PRRTE functionality. The test cases should be small, run quickly, and generate consistent output that can
be compared to a baseline file. Tests which rely on the specific ordering of output, such as verifying that a request from
a tool is immediately followed by a response from a server process or daemon are outside the scope of this framework.

A baseline file is a file containing stdout or stderr output from a previous run of the
test case which has been determined to be correct.

The run.py script runs each test case that is specified in the **tests** array in that script. An entry in the tests array
names a testcase, specifies flags as needed to control execution of supporting processes, and contains the command line to
run the test. The run.py script supports limited symbolic substitution, where the script looks for and handles specific
substitutions as needed.

The run.py script can launch supporting processes needed by a test, such as persistent daemons and applications
to attach to. The run.py script also attempts to enforce timeouts to avoid the script hanging due to a test case which
does not terminate in a reasonable time, and attempts to clean up all related processes at the end of each test case
execution attempt.

Timeouts in this script can be controlled by the following environment variables:
+ **TC_TIMEOUT**: Specifies the maximum number of seconds the test case is allowed to run before timeout. Default 60 seconds.
+ **TC_WAIT_TIMEOUT**: Maximum number of seconds allowed for a wait() system call to wait when attempting cleanup of a process.
Default 5 seconds.
+ **TC_DAEMON_DELAY**: Number of seconds to allow for daemon initialization after it is spawned. Default 5 seconds.


In order for the baseline comparison approach to work, application output must be identical from run to the next. However,
application output is not always identical from one run to the next. Output may contain variable information such as
PMIx namespaces, process ids and hostnames. Also, output may not be identical due to differences in ordering for output 
generated by the multiple processes involved in the test.

The output ordering problem is handled by prefixing each line of test case output with a label which consists of a tag to 
identify the generating process and a sequential output line number. This prefixing is handles in a modile, test-utils.c 
that implements wrappers for each of the stdio output functions. These wrappers prefix the application output text with
the tag then call the underlying libc function to write the output. 

The wrappers use a lock file to try to prevent output lines from multiple processes being fragmented in the output file.
The locking may affect interprocess timings. There's probably also work to be done to make this work with multiple nodes.

When run.py runs the test, it writes application stdout and stderr to separate files. After the test case completes, run.py 
sorts each of the output files, which groups output by process tag and sequence number. This ensures that the validation
logic does not need to deal with the randomness of execution of multiple processes in the test, but also eliminates
the possility of validating that output from multiple processes occurs in a specific order.

The run.py script invokes the tcfilter utility to deal with variable information like namespaces in test case output.
The tcfilter program uses regular expressions to match the variable data, extract it from the output text and rewrite
it as a consistent string.

The tcfilter utility maintains simple sequential arrays of strings to match, one per variable data class; namespace, 
pid, and hostname. As new unique strings of each class are found, they are added to the appropriate array. The re-written
text is a tag, such as **@NS&lt;n&gt;** where n is the array index of the original string.
  
Test case output may not be structured to be easily machine parsable. Therefore, consideration should be given to
structuring that output so it can be more reliably parsed.
